{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  #匯入Sequential模型和Dense層\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')        #df means dataframe\n",
    "dataset = df.values          #以矩陣形式呈現每一列\n",
    "np.random.shuffle(dataset)   #亂數打散資料順序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, 0:8]   #前8個欄位為特徵資料(資料共有9行)\n",
    "Y = dataset[:, 8]     #最後1個欄位為目標標籤資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()   #建立Sequential物件\n",
    "model.add(Dense(10, input_shape=(8,), activation=\"relu\"))   #unit,input_shape,activation分別代表10個神經元、輸入資料維度15、激活函數用relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(8, activation=\"relu\"))   #新增第2層隱藏層，8個神經元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation=\"sigmoid\")) #輸出層，Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])   #編譯模型         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "200/200 [==============================] - 0s 491us/step - loss: 5.1165 - accuracy: 0.6383\n",
      "Epoch 2/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.6401 - accuracy: 0.6617\n",
      "Epoch 3/150\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.5952 - accuracy: 0.6825\n",
      "Epoch 4/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.6062 - accuracy: 0.6694\n",
      "Epoch 5/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.6015 - accuracy: 0.6763\n",
      "Epoch 6/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.6006 - accuracy: 0.6802\n",
      "Epoch 7/150\n",
      "200/200 [==============================] - 0s 444us/step - loss: 0.6111 - accuracy: 0.6696\n",
      "Epoch 8/150\n",
      "200/200 [==============================] - 0s 456us/step - loss: 0.5910 - accuracy: 0.6834\n",
      "Epoch 9/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5939 - accuracy: 0.6793\n",
      "Epoch 10/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5863 - accuracy: 0.7075\n",
      "Epoch 11/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5943 - accuracy: 0.6729\n",
      "Epoch 12/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5935 - accuracy: 0.6954\n",
      "Epoch 13/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5952 - accuracy: 0.6849\n",
      "Epoch 14/150\n",
      "200/200 [==============================] - 0s 456us/step - loss: 0.5860 - accuracy: 0.6805\n",
      "Epoch 15/150\n",
      "200/200 [==============================] - 0s 456us/step - loss: 0.5984 - accuracy: 0.6814\n",
      "Epoch 16/150\n",
      "200/200 [==============================] - 0s 424us/step - loss: 0.5867 - accuracy: 0.6829\n",
      "Epoch 17/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5838 - accuracy: 0.6898\n",
      "Epoch 18/150\n",
      "200/200 [==============================] - 0s 456us/step - loss: 0.5781 - accuracy: 0.7025\n",
      "Epoch 19/150\n",
      "200/200 [==============================] - 0s 461us/step - loss: 0.5886 - accuracy: 0.6875\n",
      "Epoch 20/150\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.5804 - accuracy: 0.6994\n",
      "Epoch 21/150\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.5808 - accuracy: 0.6951\n",
      "Epoch 22/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5901 - accuracy: 0.6902\n",
      "Epoch 23/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5741 - accuracy: 0.7066\n",
      "Epoch 24/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5828 - accuracy: 0.6949\n",
      "Epoch 25/150\n",
      "200/200 [==============================] - 0s 481us/step - loss: 0.5981 - accuracy: 0.6823\n",
      "Epoch 26/150\n",
      "200/200 [==============================] - 0s 491us/step - loss: 0.5750 - accuracy: 0.7080\n",
      "Epoch 27/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5650 - accuracy: 0.7041\n",
      "Epoch 28/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5785 - accuracy: 0.7005\n",
      "Epoch 29/150\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.5828 - accuracy: 0.7006\n",
      "Epoch 30/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5851 - accuracy: 0.7002\n",
      "Epoch 31/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5802 - accuracy: 0.7029\n",
      "Epoch 32/150\n",
      "200/200 [==============================] - 0s 434us/step - loss: 0.5809 - accuracy: 0.7011\n",
      "Epoch 33/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5652 - accuracy: 0.7146\n",
      "Epoch 34/150\n",
      "200/200 [==============================] - 0s 425us/step - loss: 0.5887 - accuracy: 0.6939\n",
      "Epoch 35/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5722 - accuracy: 0.7016\n",
      "Epoch 36/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5897 - accuracy: 0.6817\n",
      "Epoch 37/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5712 - accuracy: 0.6912\n",
      "Epoch 38/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5693 - accuracy: 0.7118\n",
      "Epoch 39/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5764 - accuracy: 0.6995\n",
      "Epoch 40/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5798 - accuracy: 0.7004\n",
      "Epoch 41/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5775 - accuracy: 0.7112\n",
      "Epoch 42/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5746 - accuracy: 0.6927\n",
      "Epoch 43/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5648 - accuracy: 0.7274\n",
      "Epoch 44/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5770 - accuracy: 0.7116\n",
      "Epoch 45/150\n",
      "200/200 [==============================] - 0s 555us/step - loss: 0.5847 - accuracy: 0.6977\n",
      "Epoch 46/150\n",
      "200/200 [==============================] - 0s 556us/step - loss: 0.5787 - accuracy: 0.7079\n",
      "Epoch 47/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5825 - accuracy: 0.6870\n",
      "Epoch 48/150\n",
      "200/200 [==============================] - 0s 435us/step - loss: 0.5769 - accuracy: 0.7044\n",
      "Epoch 49/150\n",
      "200/200 [==============================] - 0s 435us/step - loss: 0.5878 - accuracy: 0.6935\n",
      "Epoch 50/150\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.5792 - accuracy: 0.6805\n",
      "Epoch 51/150\n",
      "200/200 [==============================] - 0s 477us/step - loss: 0.5714 - accuracy: 0.7015\n",
      "Epoch 52/150\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.5605 - accuracy: 0.7267\n",
      "Epoch 53/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5619 - accuracy: 0.7128\n",
      "Epoch 54/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5827 - accuracy: 0.7024\n",
      "Epoch 55/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5544 - accuracy: 0.7154\n",
      "Epoch 56/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5747 - accuracy: 0.7073\n",
      "Epoch 57/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5761 - accuracy: 0.7121\n",
      "Epoch 58/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5745 - accuracy: 0.7092\n",
      "Epoch 59/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5506 - accuracy: 0.7237\n",
      "Epoch 60/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5636 - accuracy: 0.6984\n",
      "Epoch 61/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5758 - accuracy: 0.6973\n",
      "Epoch 62/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5738 - accuracy: 0.7065\n",
      "Epoch 63/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5713 - accuracy: 0.7065\n",
      "Epoch 64/150\n",
      "200/200 [==============================] - 0s 468us/step - loss: 0.5555 - accuracy: 0.7160\n",
      "Epoch 65/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5520 - accuracy: 0.7202\n",
      "Epoch 66/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5573 - accuracy: 0.7087\n",
      "Epoch 67/150\n",
      "200/200 [==============================] - 0s 432us/step - loss: 0.5777 - accuracy: 0.6961\n",
      "Epoch 68/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5681 - accuracy: 0.7017\n",
      "Epoch 69/150\n",
      "200/200 [==============================] - 0s 439us/step - loss: 0.5730 - accuracy: 0.7146\n",
      "Epoch 70/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5623 - accuracy: 0.7162\n",
      "Epoch 71/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5781 - accuracy: 0.7037\n",
      "Epoch 72/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5546 - accuracy: 0.7250\n",
      "Epoch 73/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5718 - accuracy: 0.7019\n",
      "Epoch 74/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5832 - accuracy: 0.6975\n",
      "Epoch 75/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5845 - accuracy: 0.6855\n",
      "Epoch 76/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5692 - accuracy: 0.7001\n",
      "Epoch 77/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5707 - accuracy: 0.7148\n",
      "Epoch 78/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5600 - accuracy: 0.7179\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 436us/step - loss: 0.5741 - accuracy: 0.7036\n",
      "Epoch 80/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5636 - accuracy: 0.7016\n",
      "Epoch 81/150\n",
      "200/200 [==============================] - 0s 418us/step - loss: 0.5739 - accuracy: 0.7091\n",
      "Epoch 82/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5682 - accuracy: 0.7012\n",
      "Epoch 83/150\n",
      "200/200 [==============================] - 0s 437us/step - loss: 0.5822 - accuracy: 0.6998\n",
      "Epoch 84/150\n",
      "200/200 [==============================] - 0s 424us/step - loss: 0.5698 - accuracy: 0.6985\n",
      "Epoch 85/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5718 - accuracy: 0.6947\n",
      "Epoch 86/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5675 - accuracy: 0.7265\n",
      "Epoch 87/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5748 - accuracy: 0.7118\n",
      "Epoch 88/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5605 - accuracy: 0.7207\n",
      "Epoch 89/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5681 - accuracy: 0.7014\n",
      "Epoch 90/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5681 - accuracy: 0.7075\n",
      "Epoch 91/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5622 - accuracy: 0.7059\n",
      "Epoch 92/150\n",
      "200/200 [==============================] - 0s 496us/step - loss: 0.5790 - accuracy: 0.7002\n",
      "Epoch 93/150\n",
      "200/200 [==============================] - 0s 496us/step - loss: 0.5414 - accuracy: 0.7227\n",
      "Epoch 94/150\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.5861 - accuracy: 0.6892\n",
      "Epoch 95/150\n",
      "200/200 [==============================] - 0s 461us/step - loss: 0.5668 - accuracy: 0.7195\n",
      "Epoch 96/150\n",
      "200/200 [==============================] - 0s 476us/step - loss: 0.5671 - accuracy: 0.6942\n",
      "Epoch 97/150\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.5741 - accuracy: 0.6949\n",
      "Epoch 98/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5726 - accuracy: 0.7026\n",
      "Epoch 99/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5748 - accuracy: 0.7072\n",
      "Epoch 100/150\n",
      "200/200 [==============================] - 0s 440us/step - loss: 0.5964 - accuracy: 0.6757\n",
      "Epoch 101/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5776 - accuracy: 0.7014\n",
      "Epoch 102/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5732 - accuracy: 0.6944\n",
      "Epoch 103/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5713 - accuracy: 0.7045\n",
      "Epoch 104/150\n",
      "200/200 [==============================] - 0s 420us/step - loss: 0.5840 - accuracy: 0.6862\n",
      "Epoch 105/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5728 - accuracy: 0.7017\n",
      "Epoch 106/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5639 - accuracy: 0.7136\n",
      "Epoch 107/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5719 - accuracy: 0.7092\n",
      "Epoch 108/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5658 - accuracy: 0.7020\n",
      "Epoch 109/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5785 - accuracy: 0.6965\n",
      "Epoch 110/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5550 - accuracy: 0.7146\n",
      "Epoch 111/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5691 - accuracy: 0.7046\n",
      "Epoch 112/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5448 - accuracy: 0.7359\n",
      "Epoch 113/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5701 - accuracy: 0.6926\n",
      "Epoch 114/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5741 - accuracy: 0.7061\n",
      "Epoch 115/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5781 - accuracy: 0.6780\n",
      "Epoch 116/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5669 - accuracy: 0.7152\n",
      "Epoch 117/150\n",
      "200/200 [==============================] - 0s 445us/step - loss: 0.5828 - accuracy: 0.7020\n",
      "Epoch 118/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5694 - accuracy: 0.7109\n",
      "Epoch 119/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5714 - accuracy: 0.7094\n",
      "Epoch 120/150\n",
      "200/200 [==============================] - 0s 430us/step - loss: 0.5717 - accuracy: 0.7100\n",
      "Epoch 121/150\n",
      "200/200 [==============================] - 0s 424us/step - loss: 0.5750 - accuracy: 0.6953\n",
      "Epoch 122/150\n",
      "200/200 [==============================] - 0s 421us/step - loss: 0.5577 - accuracy: 0.7069\n",
      "Epoch 123/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5803 - accuracy: 0.6958\n",
      "Epoch 124/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5702 - accuracy: 0.7045\n",
      "Epoch 125/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5590 - accuracy: 0.7089\n",
      "Epoch 126/150\n",
      "200/200 [==============================] - 0s 476us/step - loss: 0.5723 - accuracy: 0.6998\n",
      "Epoch 127/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5691 - accuracy: 0.7146\n",
      "Epoch 128/150\n",
      "200/200 [==============================] - 0s 466us/step - loss: 0.5535 - accuracy: 0.7132\n",
      "Epoch 129/150\n",
      "200/200 [==============================] - 0s 444us/step - loss: 0.5537 - accuracy: 0.7160\n",
      "Epoch 130/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5623 - accuracy: 0.7134\n",
      "Epoch 131/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5669 - accuracy: 0.7029\n",
      "Epoch 132/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5698 - accuracy: 0.7072\n",
      "Epoch 133/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5713 - accuracy: 0.7013\n",
      "Epoch 134/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5780 - accuracy: 0.7002\n",
      "Epoch 135/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5605 - accuracy: 0.7128\n",
      "Epoch 136/150\n",
      "200/200 [==============================] - 0s 424us/step - loss: 0.5676 - accuracy: 0.7164\n",
      "Epoch 137/150\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.5806 - accuracy: 0.6881\n",
      "Epoch 138/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5582 - accuracy: 0.7160\n",
      "Epoch 139/150\n",
      "200/200 [==============================] - 0s 441us/step - loss: 0.5757 - accuracy: 0.6940\n",
      "Epoch 140/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5578 - accuracy: 0.7008\n",
      "Epoch 141/150\n",
      "200/200 [==============================] - 0s 434us/step - loss: 0.5665 - accuracy: 0.7127\n",
      "Epoch 142/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5712 - accuracy: 0.7060\n",
      "Epoch 143/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5942 - accuracy: 0.6882\n",
      "Epoch 144/150\n",
      "200/200 [==============================] - 0s 436us/step - loss: 0.5614 - accuracy: 0.7175\n",
      "Epoch 145/150\n",
      "200/200 [==============================] - 0s 446us/step - loss: 0.5611 - accuracy: 0.7082\n",
      "Epoch 146/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5735 - accuracy: 0.6989\n",
      "Epoch 147/150\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.5717 - accuracy: 0.7091\n",
      "Epoch 148/150\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.5625 - accuracy: 0.7084\n",
      "Epoch 149/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5655 - accuracy: 0.7125\n",
      "Epoch 150/150\n",
      "200/200 [==============================] - 0s 431us/step - loss: 0.5682 - accuracy: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181cdd4b1f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=150, batch_size=10)     #訓練模型（單次epoch=(全部訓練樣本/batchsize)/iteration=1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 579us/step - loss: 0.5708 - accuracy: 0.7030\n",
      "loss = 0.57\n",
      "準確度 = 0.70\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= X.mean(axis=0)     #先減掉平均值，再除以標準差，可以將數值移位成平均值0，標準差1的資料分布\n",
    "X /= X.std(axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181cdce1c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)      #verbose=0不顯示訓練狀況；=1會顯示所有損失和準確度；=2會顯示每一次訓練週期的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 540us/step - loss: 0.3597 - accuracy: 0.8350\n",
      "loss = 0.36\n",
      "準確度 = 0.83\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度提升到0.79！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical  #進行 One-hot Encoding\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(8, ), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))          #改用softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 563us/step - loss: 0.3880 - accuracy: 0.8210\n",
      "loss = 0.39\n",
      "準確度 = 0.82\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)      #verbose=0不顯示訓練狀況；=1會顯示所有損失和準確度；=2會顯示每一次訓練週期的資料\n",
    "\n",
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度提升0.01至0.80！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用初始器\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#kernal_initializer初始神經層的權重矩陣、bias_initializer初始偏向量的值(zeros代表0,ones代表1)\n",
    "\n",
    "model.add(Dense(10, input_shape=(8, ),kernel_initializer=\"random_uniform\",bias_initializer=\"ones\",activation=\"relu\"))\n",
    "model.add(Dense(8, input_shape=(8, ),kernel_initializer=\"random_uniform\",bias_initializer=\"ones\",activation=\"relu\"))\n",
    "model.add(Dense(2, input_shape=(8, ),kernel_initializer=\"random_uniform\",bias_initializer=\"ones\",activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 627us/step - loss: 0.4395 - accuracy: 0.7840\n",
      "loss = 0.44\n",
      "準確度 = 0.78\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)      #verbose=0不顯示訓練狀況；=1會顯示所有損失和準確度；=2會顯示每一次訓練週期的資料\n",
    "\n",
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.78！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 515us/step - loss: 0.3804 - accuracy: 0.8325\n",
      "loss = 0.38\n",
      "準確度 = 0.83\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  #優化器改成adam\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)      #verbose=0不顯示訓練狀況；=1會顯示所有損失和準確度；=2會顯示每一次訓練週期的資料\n",
    "\n",
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.80！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 66        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 306\n",
      "Trainable params: 306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(10, input_shape=(8, ),activation=\"relu\"))\n",
    "model.add(Dense(6,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 579us/step - loss: 0.3642 - accuracy: 0.8420\n",
      "loss = 0.36\n",
      "準確度 = 0.84\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  #優化器改成adam\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)      #verbose=0不顯示訓練狀況；=1會顯示所有損失和準確度；=2會顯示每一次訓練週期的資料\n",
    "\n",
    "loss, accuracy = model.evaluate(X, Y)         \n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.81！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = X[:690], Y[:690] #前690筆訓練資料\n",
    "X_test, Y_test = X[690:], Y[690:] #後78筆訓練資料       #使用df.shape可看到資料總筆數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181cf455820>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 616us/step - loss: 0.3406 - accuracy: 0.8667\n",
      "訓練資料集資訊：\n",
      "loss = 0.34\n",
      "準確度 = 0.87\n",
      "-------------------------------------------\n",
      "41/41 [==============================] - 0s 598us/step - loss: 0.3939 - accuracy: 0.8397\n",
      "測試資料集資訊：\n",
      "loss = 0.39\n",
      "準確度 = 0.84\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train)         \n",
    "print(\"訓練資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.83！\n",
    "print(\"-------------------------------------------\")\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"測試資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.81！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.3477 - accuracy: 0.8551 - val_loss: 0.3856 - val_accuracy: 0.8351\n",
      "Epoch 2/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8667 - val_loss: 0.3867 - val_accuracy: 0.8359\n",
      "Epoch 3/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8681 - val_loss: 0.3870 - val_accuracy: 0.8351\n",
      "Epoch 4/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8623 - val_loss: 0.3922 - val_accuracy: 0.8328\n",
      "Epoch 5/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8551 - val_loss: 0.3995 - val_accuracy: 0.8351\n",
      "Epoch 6/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8594 - val_loss: 0.3906 - val_accuracy: 0.8458\n",
      "Epoch 7/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8580 - val_loss: 0.3823 - val_accuracy: 0.8504\n",
      "Epoch 8/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8652 - val_loss: 0.3819 - val_accuracy: 0.8397\n",
      "Epoch 9/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8565 - val_loss: 0.3839 - val_accuracy: 0.8313\n",
      "Epoch 10/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8522 - val_loss: 0.3860 - val_accuracy: 0.8458\n",
      "Epoch 11/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8638 - val_loss: 0.3864 - val_accuracy: 0.8382\n",
      "Epoch 12/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8580 - val_loss: 0.3866 - val_accuracy: 0.8366\n",
      "Epoch 13/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8565 - val_loss: 0.3871 - val_accuracy: 0.8366\n",
      "Epoch 14/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.8507 - val_loss: 0.4021 - val_accuracy: 0.8344\n",
      "Epoch 15/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8594 - val_loss: 0.3848 - val_accuracy: 0.8374\n",
      "Epoch 16/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8609 - val_loss: 0.3894 - val_accuracy: 0.8321\n",
      "Epoch 17/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8565 - val_loss: 0.3895 - val_accuracy: 0.8374\n",
      "Epoch 18/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8565 - val_loss: 0.3855 - val_accuracy: 0.8397\n",
      "Epoch 19/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8638 - val_loss: 0.3915 - val_accuracy: 0.8313\n",
      "Epoch 20/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8609 - val_loss: 0.3962 - val_accuracy: 0.8267\n",
      "Epoch 21/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8696 - val_loss: 0.3899 - val_accuracy: 0.8389\n",
      "Epoch 22/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8551 - val_loss: 0.4022 - val_accuracy: 0.8321\n",
      "Epoch 23/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8667 - val_loss: 0.3961 - val_accuracy: 0.8366\n",
      "Epoch 24/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8638 - val_loss: 0.3980 - val_accuracy: 0.8260\n",
      "Epoch 25/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8580 - val_loss: 0.3885 - val_accuracy: 0.8389\n",
      "Epoch 26/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8652 - val_loss: 0.3851 - val_accuracy: 0.8344\n",
      "Epoch 27/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8638 - val_loss: 0.3914 - val_accuracy: 0.8359\n",
      "Epoch 28/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8609 - val_loss: 0.3903 - val_accuracy: 0.8359\n",
      "Epoch 29/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8609 - val_loss: 0.3822 - val_accuracy: 0.8328\n",
      "Epoch 30/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8652 - val_loss: 0.3945 - val_accuracy: 0.8260\n",
      "Epoch 31/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8652 - val_loss: 0.3937 - val_accuracy: 0.8321\n",
      "Epoch 32/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8681 - val_loss: 0.3867 - val_accuracy: 0.8336\n",
      "Epoch 33/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8667 - val_loss: 0.3873 - val_accuracy: 0.8366\n",
      "Epoch 34/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8681 - val_loss: 0.3960 - val_accuracy: 0.8282\n",
      "Epoch 35/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8580 - val_loss: 0.3948 - val_accuracy: 0.8397\n",
      "Epoch 36/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8667 - val_loss: 0.3869 - val_accuracy: 0.8389\n",
      "Epoch 37/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8565 - val_loss: 0.3875 - val_accuracy: 0.8405\n",
      "Epoch 38/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8638 - val_loss: 0.3899 - val_accuracy: 0.8374\n",
      "Epoch 39/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8638 - val_loss: 0.3879 - val_accuracy: 0.8366\n",
      "Epoch 40/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8609 - val_loss: 0.3882 - val_accuracy: 0.8290\n",
      "Epoch 41/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8710 - val_loss: 0.3902 - val_accuracy: 0.8344\n",
      "Epoch 42/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8638 - val_loss: 0.3844 - val_accuracy: 0.8344\n",
      "Epoch 43/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8638 - val_loss: 0.3976 - val_accuracy: 0.8298\n",
      "Epoch 44/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8667 - val_loss: 0.3872 - val_accuracy: 0.8405\n",
      "Epoch 45/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8609 - val_loss: 0.3875 - val_accuracy: 0.8359\n",
      "Epoch 46/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8652 - val_loss: 0.3861 - val_accuracy: 0.8344\n",
      "Epoch 47/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8681 - val_loss: 0.4052 - val_accuracy: 0.8328\n",
      "Epoch 48/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8638 - val_loss: 0.3887 - val_accuracy: 0.8374\n",
      "Epoch 49/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8580 - val_loss: 0.4098 - val_accuracy: 0.8267\n",
      "Epoch 50/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8710 - val_loss: 0.3881 - val_accuracy: 0.8305\n",
      "Epoch 51/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8638 - val_loss: 0.3886 - val_accuracy: 0.8298\n",
      "Epoch 52/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8652 - val_loss: 0.3871 - val_accuracy: 0.8443\n",
      "Epoch 53/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8609 - val_loss: 0.3881 - val_accuracy: 0.8313\n",
      "Epoch 54/150\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8652 - val_loss: 0.3940 - val_accuracy: 0.8351\n",
      "Epoch 55/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8623 - val_loss: 0.3902 - val_accuracy: 0.8351\n",
      "Epoch 56/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8739 - val_loss: 0.4093 - val_accuracy: 0.8275\n",
      "Epoch 57/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8725 - val_loss: 0.4108 - val_accuracy: 0.8351\n",
      "Epoch 58/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8725 - val_loss: 0.3971 - val_accuracy: 0.8298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8652 - val_loss: 0.3910 - val_accuracy: 0.8282\n",
      "Epoch 60/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8696 - val_loss: 0.3924 - val_accuracy: 0.8382\n",
      "Epoch 61/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8594 - val_loss: 0.3900 - val_accuracy: 0.8374\n",
      "Epoch 62/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8623 - val_loss: 0.3917 - val_accuracy: 0.8359\n",
      "Epoch 63/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8551 - val_loss: 0.3867 - val_accuracy: 0.8321\n",
      "Epoch 64/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8739 - val_loss: 0.3887 - val_accuracy: 0.8382\n",
      "Epoch 65/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8768 - val_loss: 0.3903 - val_accuracy: 0.8313\n",
      "Epoch 66/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8710 - val_loss: 0.3862 - val_accuracy: 0.8321\n",
      "Epoch 67/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8681 - val_loss: 0.3865 - val_accuracy: 0.8435\n",
      "Epoch 68/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8623 - val_loss: 0.3954 - val_accuracy: 0.8298\n",
      "Epoch 69/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8681 - val_loss: 0.3915 - val_accuracy: 0.8382\n",
      "Epoch 70/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8493 - val_loss: 0.3977 - val_accuracy: 0.8237\n",
      "Epoch 71/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8580 - val_loss: 0.3888 - val_accuracy: 0.8344\n",
      "Epoch 72/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8551 - val_loss: 0.3911 - val_accuracy: 0.8282\n",
      "Epoch 73/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8638 - val_loss: 0.3859 - val_accuracy: 0.8412\n",
      "Epoch 74/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8710 - val_loss: 0.4060 - val_accuracy: 0.8336\n",
      "Epoch 75/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8609 - val_loss: 0.3874 - val_accuracy: 0.8328\n",
      "Epoch 76/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8696 - val_loss: 0.3904 - val_accuracy: 0.8351\n",
      "Epoch 77/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8638 - val_loss: 0.3881 - val_accuracy: 0.8366\n",
      "Epoch 78/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8725 - val_loss: 0.3879 - val_accuracy: 0.8351\n",
      "Epoch 79/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8565 - val_loss: 0.4177 - val_accuracy: 0.8275\n",
      "Epoch 80/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8638 - val_loss: 0.3881 - val_accuracy: 0.8321\n",
      "Epoch 81/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8594 - val_loss: 0.3903 - val_accuracy: 0.8374\n",
      "Epoch 82/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8725 - val_loss: 0.4044 - val_accuracy: 0.8244\n",
      "Epoch 83/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8580 - val_loss: 0.3880 - val_accuracy: 0.8427\n",
      "Epoch 84/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8681 - val_loss: 0.3919 - val_accuracy: 0.8328\n",
      "Epoch 85/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8638 - val_loss: 0.3935 - val_accuracy: 0.8344\n",
      "Epoch 86/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8623 - val_loss: 0.3944 - val_accuracy: 0.8313\n",
      "Epoch 87/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8696 - val_loss: 0.3880 - val_accuracy: 0.8382\n",
      "Epoch 88/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8681 - val_loss: 0.3901 - val_accuracy: 0.8321\n",
      "Epoch 89/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8681 - val_loss: 0.3896 - val_accuracy: 0.8389\n",
      "Epoch 90/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8681 - val_loss: 0.3841 - val_accuracy: 0.8344\n",
      "Epoch 91/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8638 - val_loss: 0.3935 - val_accuracy: 0.8450\n",
      "Epoch 92/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8638 - val_loss: 0.3857 - val_accuracy: 0.8344\n",
      "Epoch 93/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8609 - val_loss: 0.4140 - val_accuracy: 0.8168\n",
      "Epoch 94/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8667 - val_loss: 0.3966 - val_accuracy: 0.8336\n",
      "Epoch 95/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8652 - val_loss: 0.3929 - val_accuracy: 0.8282\n",
      "Epoch 96/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8638 - val_loss: 0.4011 - val_accuracy: 0.8336\n",
      "Epoch 97/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8638 - val_loss: 0.3839 - val_accuracy: 0.8397\n",
      "Epoch 98/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8623 - val_loss: 0.3865 - val_accuracy: 0.8321\n",
      "Epoch 99/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8725 - val_loss: 0.3867 - val_accuracy: 0.8290\n",
      "Epoch 100/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8652 - val_loss: 0.3858 - val_accuracy: 0.8397\n",
      "Epoch 101/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8681 - val_loss: 0.3918 - val_accuracy: 0.8328\n",
      "Epoch 102/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8652 - val_loss: 0.4027 - val_accuracy: 0.8359\n",
      "Epoch 103/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8609 - val_loss: 0.3910 - val_accuracy: 0.8374\n",
      "Epoch 104/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8667 - val_loss: 0.3902 - val_accuracy: 0.8344\n",
      "Epoch 105/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8623 - val_loss: 0.3890 - val_accuracy: 0.8374\n",
      "Epoch 106/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8696 - val_loss: 0.3906 - val_accuracy: 0.8374\n",
      "Epoch 107/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8754 - val_loss: 0.3941 - val_accuracy: 0.8267\n",
      "Epoch 108/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8667 - val_loss: 0.3912 - val_accuracy: 0.8344\n",
      "Epoch 109/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8667 - val_loss: 0.3866 - val_accuracy: 0.8336\n",
      "Epoch 110/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8638 - val_loss: 0.3852 - val_accuracy: 0.8359\n",
      "Epoch 111/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8667 - val_loss: 0.3907 - val_accuracy: 0.8351\n",
      "Epoch 112/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8638 - val_loss: 0.3860 - val_accuracy: 0.8412\n",
      "Epoch 113/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8681 - val_loss: 0.4092 - val_accuracy: 0.8374\n",
      "Epoch 114/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8797 - val_loss: 0.3942 - val_accuracy: 0.8313\n",
      "Epoch 115/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8652 - val_loss: 0.3989 - val_accuracy: 0.8397\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8638 - val_loss: 0.3901 - val_accuracy: 0.8344\n",
      "Epoch 117/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8739 - val_loss: 0.3864 - val_accuracy: 0.8336\n",
      "Epoch 118/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8609 - val_loss: 0.3840 - val_accuracy: 0.8389\n",
      "Epoch 119/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8681 - val_loss: 0.3907 - val_accuracy: 0.8397\n",
      "Epoch 120/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8667 - val_loss: 0.3864 - val_accuracy: 0.8328\n",
      "Epoch 121/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8681 - val_loss: 0.3918 - val_accuracy: 0.8389\n",
      "Epoch 122/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8652 - val_loss: 0.3917 - val_accuracy: 0.8382\n",
      "Epoch 123/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8652 - val_loss: 0.3968 - val_accuracy: 0.8321\n",
      "Epoch 124/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8681 - val_loss: 0.4029 - val_accuracy: 0.8298\n",
      "Epoch 125/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8681 - val_loss: 0.3872 - val_accuracy: 0.8351\n",
      "Epoch 126/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8638 - val_loss: 0.3896 - val_accuracy: 0.8344\n",
      "Epoch 127/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8754 - val_loss: 0.3835 - val_accuracy: 0.8359\n",
      "Epoch 128/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8652 - val_loss: 0.3892 - val_accuracy: 0.8359\n",
      "Epoch 129/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8696 - val_loss: 0.3860 - val_accuracy: 0.8351\n",
      "Epoch 130/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8681 - val_loss: 0.3910 - val_accuracy: 0.8321\n",
      "Epoch 131/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8681 - val_loss: 0.3905 - val_accuracy: 0.8359\n",
      "Epoch 132/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8667 - val_loss: 0.4050 - val_accuracy: 0.8305\n",
      "Epoch 133/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8681 - val_loss: 0.3902 - val_accuracy: 0.8366\n",
      "Epoch 134/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8652 - val_loss: 0.3915 - val_accuracy: 0.8328\n",
      "Epoch 135/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8696 - val_loss: 0.3937 - val_accuracy: 0.8336\n",
      "Epoch 136/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8754 - val_loss: 0.3863 - val_accuracy: 0.8359\n",
      "Epoch 137/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8623 - val_loss: 0.3898 - val_accuracy: 0.8397\n",
      "Epoch 138/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8681 - val_loss: 0.3879 - val_accuracy: 0.8282\n",
      "Epoch 139/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8609 - val_loss: 0.3873 - val_accuracy: 0.8382\n",
      "Epoch 140/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8652 - val_loss: 0.3848 - val_accuracy: 0.8420\n",
      "Epoch 141/150\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8739 - val_loss: 0.3958 - val_accuracy: 0.8313\n",
      "Epoch 142/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8739 - val_loss: 0.3864 - val_accuracy: 0.8290\n",
      "Epoch 143/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8681 - val_loss: 0.3893 - val_accuracy: 0.8344\n",
      "Epoch 144/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8754 - val_loss: 0.3837 - val_accuracy: 0.8305\n",
      "Epoch 145/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8725 - val_loss: 0.3876 - val_accuracy: 0.8389\n",
      "Epoch 146/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8696 - val_loss: 0.4070 - val_accuracy: 0.8282\n",
      "Epoch 147/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8768 - val_loss: 0.3861 - val_accuracy: 0.8382\n",
      "Epoch 148/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8725 - val_loss: 0.3891 - val_accuracy: 0.8382\n",
      "Epoch 149/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8768 - val_loss: 0.3975 - val_accuracy: 0.8244\n",
      "Epoch 150/150\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8594 - val_loss: 0.3872 - val_accuracy: 0.8519\n"
     ]
    }
   ],
   "source": [
    "#驗證資料集\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data = (X_test, Y_test),\n",
    "                    epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 570us/step - loss: 0.3303 - accuracy: 0.8812\n",
      "訓練資料集資訊：\n",
      "loss = 0.33\n",
      "準確度 = 0.88\n",
      "-------------------------------------------\n",
      "41/41 [==============================] - 0s 598us/step - loss: 0.3872 - accuracy: 0.8519\n",
      "測試資料集資訊：\n",
      "loss = 0.39\n",
      "準確度 = 0.85\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train)         \n",
    "print(\"訓練資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.84！\n",
    "print(\"-------------------------------------------\")\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"測試資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.79！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history[\"loss\"]\n",
    "epochs = range(1, len(loss)+1)\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.plot(epochs, loss, \"bo\", label =\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, \"r\", label =\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-89a20196b5f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"Training Acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r--\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation Acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "acc = history.history[\"acc\"]\n",
    "epochs = range(1, len(acc)+1)\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "plt.plot(epochs, acc, \"b-\", label =\"Training Acc\")\n",
    "plt.plot(epochs, val_acc, \"r--\", label =\"Validation Acc\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8768 - val_loss: 0.3153 - val_accuracy: 0.8768\n",
      "Epoch 2/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3343 - accuracy: 0.8786 - val_loss: 0.3534 - val_accuracy: 0.8261\n",
      "Epoch 3/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8678 - val_loss: 0.3273 - val_accuracy: 0.8478\n",
      "Epoch 4/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8768 - val_loss: 0.3292 - val_accuracy: 0.8696\n",
      "Epoch 5/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8732 - val_loss: 0.3271 - val_accuracy: 0.8768\n",
      "Epoch 6/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8641 - val_loss: 0.3274 - val_accuracy: 0.8551\n",
      "Epoch 7/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8696 - val_loss: 0.3350 - val_accuracy: 0.8261\n",
      "Epoch 8/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8696 - val_loss: 0.3369 - val_accuracy: 0.8406\n",
      "Epoch 9/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8678 - val_loss: 0.3356 - val_accuracy: 0.8623\n",
      "Epoch 10/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8569 - val_loss: 0.3321 - val_accuracy: 0.8551\n",
      "Epoch 11/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8732 - val_loss: 0.3453 - val_accuracy: 0.8623\n",
      "Epoch 12/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8714 - val_loss: 0.3382 - val_accuracy: 0.8696\n",
      "Epoch 13/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8768 - val_loss: 0.3430 - val_accuracy: 0.8768\n",
      "Epoch 14/14\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8659 - val_loss: 0.3475 - val_accuracy: 0.8551\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_split=0.2,   #自動切割成80:20\n",
    "                    epochs=14, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 570us/step - loss: 0.3244 - accuracy: 0.8725\n",
      "訓練資料集資訊：\n",
      "loss = 0.32\n",
      "準確度 = 0.87\n",
      "-------------------------------------------\n",
      "41/41 [==============================] - 0s 499us/step - loss: 0.3804 - accuracy: 0.8366\n",
      "測試資料集資訊：\n",
      "loss = 0.38\n",
      "準確度 = 0.84\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train)         \n",
    "print(\"訓練資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.82！\n",
    "print(\"-------------------------------------------\")\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"測試資料集資訊：\")\n",
    "print(\"loss = {:.2f}\".format(loss))\n",
    "print(\"準確度 = {:.2f}\".format(accuracy))      #準確度0.81！\n",
    "\n",
    "#Training loss和Validation loss相當接近，準確度只差0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 665us/step - loss: 0.4578 - accuracy: 0.7783\n",
      "訓練資料集的準確度 = 0.78\n",
      "41/41 [==============================] - 0s 474us/step - loss: 0.4883 - accuracy: 0.7580\n",
      "測試資料集的準確度 = 0.76\n",
      "[0] [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hsieh73\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "np.random.seed(10)  # 指定亂數種子\n",
    "# 載入糖尿病資料集\n",
    "df = pd.read_csv(\"./diabetes.csv\")\n",
    "dataset = df.values\n",
    "np.random.shuffle(dataset)  # 使用亂數打亂資料\n",
    "# 分割成特徵資料和標籤資料\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# 特徵標準化\n",
    "X -= X.mean(axis=0)\n",
    "X /= X.std(axis=0)\n",
    "# 分割訓練和測試資料集\n",
    "X_train, Y_train = X[:690], Y[:690]     # 訓練資料前690筆\n",
    "X_test, Y_test = X[690:], Y[690:]       # 測試資料後78筆\n",
    "# 定義模型\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=(8,), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# 編譯模型\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "# 訓練模型\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=10, verbose=0)\n",
    "# 評估模型\n",
    "loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "print(\"訓練資料集的準確度 = {:.2f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"測試資料集的準確度 = {:.2f}\".format(accuracy))\n",
    "# 測試資料集的預測值\n",
    "Y_pred = model.predict_classes(X_test, batch_size=10, verbose=0)\n",
    "print(Y_pred[0], Y_pred[1])  #第0筆不會得糖尿病、第1筆會得糖尿病"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
